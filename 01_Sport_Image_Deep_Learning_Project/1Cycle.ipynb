{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f046720-2548-4035-859a-559e66f5f34a",
   "metadata": {},
   "source": [
    "### 100 Sports Image Classification\n",
    "#### 스포츠 이미지 분류\n",
    "https://www.kaggle.com/datasets/gpiosenka/sports-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "152c4585-5167-443f-a994-0e0331761a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "\n",
    "# 이미지가 저장되어 있는 경로 지정\n",
    "train_root = './datasets/Sports/train/'\n",
    "val_root = './datasets/Sports/val/'\n",
    "test_root = './datasets/Sports/test/'\n",
    "\n",
    "# 해당 경로를 통해 이미지 폴더를 찾아오기\n",
    "train_directories = glob(os.path.join(train_root, '*'))\n",
    "val_directories = glob(os.path.join(val_root, '*'))\n",
    "test_directories = glob(os.path.join(test_root, '*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccf367f9-4d1d-400b-89d3-9d08d997cc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 폴더 이름 저장할 초기 list 생성\n",
    "train_directory_names = []\n",
    "val_directory_names = []\n",
    "test_directory_names = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fac823b-bcd4-415f-9a86-dcce1dbe60b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for directory in train_directories:\n",
    "    # 디렉토리의 이름을 찾아와서 list에 저장\n",
    "    train_directory_names.append(directory[directory.rindex('\\\\') + 1:])\n",
    "\n",
    "for directory in val_directories:\n",
    "    # 디렉토리의 이름을 찾아와서 list에 저장\n",
    "    val_directory_names.append(directory[directory.rindex('\\\\') + 1:])\n",
    "\n",
    "for directory in test_directories:\n",
    "    # 디렉토리의 이름을 찾아와서 list에 저장\n",
    "    test_directory_names.append(directory[directory.rindex('\\\\') + 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7ab566d-513c-43e8-b8b7-35940b10ecc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name in train_directory_names:\n",
    "#     # 변경 전 디렉토리의 전체 경로\n",
    "#     old_directory = os.path.join(train_root, name)\n",
    "#     # 변경 후 디렉토리의 전체 경로\n",
    "#     new_directory = os.path.join(train_root, name.replace(' ', '_'))\n",
    "    \n",
    "#     # 디렉토리 이름 변경\n",
    "#     os.rename(old_directory, new_directory)\n",
    "    \n",
    "#     # 새 디렉토리 경로에서 파일 이름 변경\n",
    "#     for i, file_name in enumerate(os.listdir(new_directory)):\n",
    "#         # 변경 전 파일 전체 경로\n",
    "#         old_file = os.path.join(new_directory, file_name)\n",
    "#         # 변경  파일 전체 경로\n",
    "#         new_file = os.path.join(new_directory, name.replace(' ', '_') + str(i + 1) + '.png')\n",
    "        \n",
    "#         # 파일 이름 변경\n",
    "#         os.rename(old_file, new_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25dde578-4cc1-4bfa-a017-97b9198e9134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name in val_directory_names:\n",
    "#     # 변경 전 디렉토리의 전체 경로\n",
    "#     old_directory = os.path.join(val_root, name)\n",
    "#     # 변경 후 디렉토리의 전체 경로\n",
    "#     new_directory = os.path.join(val_root, name.replace(' ', '_'))\n",
    "    \n",
    "#     # 디렉토리 이름 변경\n",
    "#     os.rename(old_directory, new_directory)\n",
    "    \n",
    "#     # 새 디렉토리 경로에서 파일 이름 변경\n",
    "#     for i, file_name in enumerate(os.listdir(new_directory)):\n",
    "#         # 변경 전 파일 전체 경로\n",
    "#         old_file = os.path.join(new_directory, file_name)\n",
    "#         # 변경  파일 전체 경로\n",
    "#         new_file = os.path.join(new_directory, name.replace(' ', '_') + str(i + 1) + '.png')\n",
    "        \n",
    "#         # 파일 이름 변경\n",
    "#         os.rename(old_file, new_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbeec95b-4ade-4d25-ade8-42ccbda6a0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name in test_directory_names:\n",
    "#     # 변경 전 디렉토리의 전체 경로\n",
    "#     old_directory = os.path.join(test_root, name)\n",
    "#     # 변경 후 디렉토리의 전체 경로\n",
    "#     new_directory = os.path.join(test_root, name.replace(' ', '_'))\n",
    "    \n",
    "#     # 디렉토리 이름 변경\n",
    "#     os.rename(old_directory, new_directory)\n",
    "    \n",
    "#     # 새 디렉토리 경로에서 파일 이름 변경\n",
    "#     for i, file_name in enumerate(os.listdir(new_directory)):\n",
    "#         # 변경 전 파일 전체 경로\n",
    "#         old_file = os.path.join(new_directory, file_name)\n",
    "#         # 변경  파일 전체 경로\n",
    "#         new_file = os.path.join(new_directory, name.replace(' ', '_') + str(i + 1) + '.png')\n",
    "        \n",
    "#         # 파일 이름 변경\n",
    "#         os.rename(old_file, new_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90a5a201-6e91-46ea-a7c1-fee5477759dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "\n",
    "# 이미지 전처리 함수 선언\n",
    "def transform(image):\n",
    "    aug = A.Compose([\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.RandomResizedCrop(height=244, width=244, scale=(0.1, 0.5), p=0.5),\n",
    "        A.OneOf([\n",
    "            A.ColorJitter(p=0.5),\n",
    "            A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5)\n",
    "        ], p=1)\n",
    "    ], p=0.5)\n",
    "\n",
    "    return aug(image=image)['image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5acad419-b8ff-42ad-af2d-3ab9be003ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13493 images belonging to 100 classes.\n",
      "Found 500 images belonging to 100 classes.\n",
      "Found 500 images belonging to 100 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# 훈련 데이터 이미지 전처리 및 픽셀 값 조정 객체 선언\n",
    "train_image_data_generator = ImageDataGenerator(preprocessing_function=transform, rescale=1./255)\n",
    "image_data_generator = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# 디렉토리에서 이미지를 가져와 배치로 변환\n",
    "train_generator = image_data_generator.flow_from_directory(train_root, target_size=(244, 244), batch_size=32, class_mode='categorical')\n",
    "val_generator = image_data_generator.flow_from_directory(val_root, target_size=(244, 244), batch_size=32, class_mode='categorical')\n",
    "test_generator = image_data_generator.flow_from_directory(test_root, target_size=(244, 244), batch_size=32, class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dcab8ac2-78de-46d0-a3e4-e3b12909a140",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
       "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
       "       68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84,\n",
       "       85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.unique(train_generator.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31ebcf7b-73f5-4f43-bb96-aa2c27a05a4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">244</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">244</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">122</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">122</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">122</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">122</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ classifierA00                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)             │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ classifierAD01 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)                 │           <span style=\"color: #00af00; text-decoration-color: #00af00\">6,500</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">10,100</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m244\u001b[0m, \u001b[38;5;34m244\u001b[0m, \u001b[38;5;34m3\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m122\u001b[0m, \u001b[38;5;34m122\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │             \u001b[38;5;34m896\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m122\u001b[0m, \u001b[38;5;34m122\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m18,496\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ classifierA00                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)             │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ classifierAD01 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)                 │           \u001b[38;5;34m6,500\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ output (\u001b[38;5;33mDense\u001b[0m)                       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)                 │          \u001b[38;5;34m10,100\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">35,992</span> (140.59 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m35,992\u001b[0m (140.59 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">35,992</span> (140.59 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m35,992\u001b[0m (140.59 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dropout, Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# 입력 데이터 크기 지정\n",
    "INPUT_SIZE = 244\n",
    "\n",
    "# Input layer\n",
    "input_tensor = Input(shape=(INPUT_SIZE, INPUT_SIZE, 3))\n",
    "\n",
    "# Convolutional Layer 1\n",
    "x = Conv2D(filters=32, kernel_size=3, strides=2, padding='same', activation='relu')(input_tensor)\n",
    "x = Conv2D(filters=64, kernel_size=3, padding='same', activation='relu')(x)\n",
    "\n",
    "# Flatten Layer: 2D 텐서를 1D로 평탄화\n",
    "x = GlobalAveragePooling2D(name='classifierA00')(x)\n",
    "\n",
    "# Dense Layer (은닉층)\n",
    "x = Dense(100, activation='relu', name='classifierAD01')(x)\n",
    "\n",
    "# Dense Layer (출력층)\n",
    "output = Dense(100, activation='softmax', name='output')(x)\n",
    "\n",
    "# 인공 신경망 생성\n",
    "model = Model(inputs=input_tensor, outputs=output)\n",
    "# 모델 구조 확인\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a942c1a-28ad-495e-9dff-f4cad5f3b0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile(optimizer=Adam(), loss=CategoricalCrossentropy(), metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc3f11ce-fa7f-4fd7-bfc9-ae61e1fc4215",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "# weights 저장\n",
    "mcp_cb = ModelCheckpoint(\n",
    "    filepath=\"./callback_files/weights.{epoch:03d}-{val_loss:.4f}-{acc:.4f}.weights.h5\",\n",
    "    monitor='val_loss',\n",
    "    save_best_only=False,\n",
    "    save_weights_only=True,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "# 일정 기간 동안 성능이 개선되지 않을 시 학습률 동적으로 감소\n",
    "rlr_cb = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.1,\n",
    "    patience=2,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "# 일정 기간 동안 성능이 개선되지 않을 시 학습 조기 중단\n",
    "ely_cb = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    "    mode='min'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac5d3182-7b94-4adf-9701-a672dfbe7406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LEGION\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m274/422\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m2:12\u001b[0m 895ms/step - acc: 0.0136 - loss: 4.5539"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": "Graph execution error:\n\nDetected at node PyFunc defined at (most recent call last):\n<stack traces unavailable>\nUnidentifiedImageError: cannot identify image file <_io.BytesIO object at 0x00000280A0F1E110>\nTraceback (most recent call last):\n\n  File \"C:\\Users\\LEGION\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 270, in __call__\n    ret = func(*args)\n          ^^^^^^^^^^^\n\n  File \"C:\\Users\\LEGION\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 643, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n\n  File \"C:\\Users\\LEGION\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\from_generator_op.py\", line 198, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"C:\\Users\\LEGION\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py\", line 260, in _get_iterator\n    for i, batch in enumerate(gen_fn()):\n\n  File \"C:\\Users\\LEGION\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py\", line 253, in generator_fn\n    yield self.py_dataset[i]\n          ~~~~~~~~~~~~~~~^^^\n\n  File \"C:\\Users\\LEGION\\anaconda3\\Lib\\site-packages\\keras\\src\\legacy\\preprocessing\\image.py\", line 68, in __getitem__\n    return self._get_batches_of_transformed_samples(index_array)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"C:\\Users\\LEGION\\anaconda3\\Lib\\site-packages\\keras\\src\\legacy\\preprocessing\\image.py\", line 313, in _get_batches_of_transformed_samples\n    img = image_utils.load_img(\n          ^^^^^^^^^^^^^^^^^^^^^\n\n  File \"C:\\Users\\LEGION\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\image_utils.py\", line 236, in load_img\n    img = pil_image.open(io.BytesIO(f.read()))\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"C:\\Users\\LEGION\\anaconda3\\Lib\\site-packages\\PIL\\Image.py\", line 3309, in open\n    raise UnidentifiedImageError(msg)\n\nPIL.UnidentifiedImageError: cannot identify image file <_io.BytesIO object at 0x00000280A0F1E110>\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]] [Op:__inference_one_step_on_iterator_1703]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 훈련\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(train_generator, \n\u001b[0;32m      3\u001b[0m                     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, \n\u001b[0;32m      4\u001b[0m                     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, \n\u001b[0;32m      5\u001b[0m                     callbacks\u001b[38;5;241m=\u001b[39m[mcp_cb, rlr_cb, ely_cb]\n\u001b[0;32m      6\u001b[0m                    )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mUnknownError\u001b[0m: Graph execution error:\n\nDetected at node PyFunc defined at (most recent call last):\n<stack traces unavailable>\nUnidentifiedImageError: cannot identify image file <_io.BytesIO object at 0x00000280A0F1E110>\nTraceback (most recent call last):\n\n  File \"C:\\Users\\LEGION\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 270, in __call__\n    ret = func(*args)\n          ^^^^^^^^^^^\n\n  File \"C:\\Users\\LEGION\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 643, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n\n  File \"C:\\Users\\LEGION\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\from_generator_op.py\", line 198, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"C:\\Users\\LEGION\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py\", line 260, in _get_iterator\n    for i, batch in enumerate(gen_fn()):\n\n  File \"C:\\Users\\LEGION\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py\", line 253, in generator_fn\n    yield self.py_dataset[i]\n          ~~~~~~~~~~~~~~~^^^\n\n  File \"C:\\Users\\LEGION\\anaconda3\\Lib\\site-packages\\keras\\src\\legacy\\preprocessing\\image.py\", line 68, in __getitem__\n    return self._get_batches_of_transformed_samples(index_array)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"C:\\Users\\LEGION\\anaconda3\\Lib\\site-packages\\keras\\src\\legacy\\preprocessing\\image.py\", line 313, in _get_batches_of_transformed_samples\n    img = image_utils.load_img(\n          ^^^^^^^^^^^^^^^^^^^^^\n\n  File \"C:\\Users\\LEGION\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\image_utils.py\", line 236, in load_img\n    img = pil_image.open(io.BytesIO(f.read()))\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"C:\\Users\\LEGION\\anaconda3\\Lib\\site-packages\\PIL\\Image.py\", line 3309, in open\n    raise UnidentifiedImageError(msg)\n\nPIL.UnidentifiedImageError: cannot identify image file <_io.BytesIO object at 0x00000280A0F1E110>\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]] [Op:__inference_one_step_on_iterator_1703]"
     ]
    }
   ],
   "source": [
    "# 훈련\n",
    "history = model.fit(train_generator, \n",
    "                    batch_size=64, \n",
    "                    epochs=20, \n",
    "                    callbacks=[mcp_cb, rlr_cb, ely_cb]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e21fd7-5524-4fd2-97dd-c15c5c23a670",
   "metadata": {},
   "source": [
    "ValueError: Argument `output` must have rank (ndim) `target.ndim - 1`. Received: target.shape=(None, 100), output.shape=(None, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c22b02-227b-48ea-af3a-f68753117d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dropout, Dense, GlobalAveragePooling2D\n",
    "# from tensorflow.keras.models import Model\n",
    "\n",
    "# # 입력 데이터 크기 지정\n",
    "# INPUT_SIZE = 244\n",
    "\n",
    "# # Input layer\n",
    "# input_tensor = Input(shape=(INPUT_SIZE, INPUT_SIZE, 3))\n",
    "\n",
    "# # Convolutional Layer 1\n",
    "# x = Conv2D(filters=128, kernel_size=3, strides=2, padding='same', activation='relu')(input_tensor)\n",
    "# x = Conv2D(filters=512, kernel_size=3, padding='same', activation='relu')(x)\n",
    "\n",
    "# # Flatten Layer: 2D 텐서를 1D로 평탄화\n",
    "# x = GlobalAveragePooling2D(name='classifierA00')(x)\n",
    "\n",
    "# # Dense Layer (은닉층)\n",
    "# x = Dense(100, activation='relu', name='classifierAD01')(x)\n",
    "\n",
    "# # Dense Layer (출력층)\n",
    "# output = Dense(100, activation='softmax', name='output')(x)\n",
    "\n",
    "# # 인공 신경망 생성\n",
    "# model = Model(inputs=input_tensor, outputs=output)\n",
    "# # 모델 구조 확인\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e06174-4c82-4f28-a8a5-2e3328bcc55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dropout, Dense, GlobalAveragePooling2D\n",
    "# from tensorflow.keras.models import Model\n",
    "\n",
    "# # 입력 데이터 크기 지정\n",
    "# INPUT_SIZE = 244\n",
    "\n",
    "# # Input layer\n",
    "# input_tensor = Input(shape=(INPUT_SIZE, INPUT_SIZE, 3))\n",
    "\n",
    "# # Convolutional Layer 1\n",
    "# x = Conv2D(filters=256, kernel_size=3, strides=2, padding='same', activation='relu')(input_tensor)\n",
    "# x = Conv2D(filters=512, kernel_size=3, padding='same', activation='relu')(x)\n",
    "\n",
    "# # Flatten Layer: 2D 텐서를 1D로 평탄화\n",
    "# x = GlobalAveragePooling2D(name='classifierA00')(x)\n",
    "\n",
    "# # Dense Layer (은닉층)\n",
    "# x = Dense(200, activation='relu', name='classifierAD01')(x)\n",
    "\n",
    "# # Dense Layer (출력층)\n",
    "# output = Dense(100, activation='softmax', name='output')(x)\n",
    "\n",
    "# # 인공 신경망 생성\n",
    "# model = Model(inputs=input_tensor, outputs=output)\n",
    "# # 모델 구조 확인\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e393c20-860b-4038-b501-6c5978df4d27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f60cea-ab16-49cf-8666-5f833bfb126e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
